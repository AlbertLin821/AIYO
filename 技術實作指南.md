# 技術實作指南

**文件版本**：1.0  
**建立日期**：2026-02-14  
**適用專案**：AIYO 愛遊互動式旅遊網站

---

## 1. 文件目的

本文件說明如何從 OpenAI API 遷移到本地部署的 Ollama/vLLM，包含模型選擇、部署架構與實作範例。

---

## 2. OpenAI API 能力需求回顧

### 2.1 目前使用 OpenAI 的主要功能

- **中文旅遊對話**：自然語言理解與生成
- **RAG 的「答案融合」**：整合檢索結果與生成回答
- **Tool-calling**：輸出 JSON 任務，如 `search_videos`, `plan_itinerary`
- **串流回覆**：即時串流生成回應

### 2.2 遷移目標

將上述功能遷移到本地部署的 LLM，降低 API 成本並提升資料隱私。

---

## 3. Ollama 可行模型（替代 OpenAI）

### 3.1 模型推薦

#### 1. Qwen3-8B-Instruct（阿里）

**優點**：

- 中文能力極佳，對話自然，旅遊領域回答好
- Tool-calling / JSON 輸出穩定
- 多個 blog 指出其在中文任務與指令跟隨上與 GPT-4o-mini 接近

**適用場景**：

- 中文對話為主
- 需要穩定的 JSON 輸出
- 旅遊領域問答

#### 2. Llama3.3-8B-Instruct（Meta）

**優點**：

- 多項 benchmark（MMLU, IFEval）接近或略高於 GPT-4o-mini
- 全能型，對工具呼叫、結構化輸出也不錯

**適用場景**：

- 需要通用能力
- 多語言支援
- 結構化輸出需求

#### 3. Mistral Nemo / Small 3.1

**優點**：

- 高 throughput、低延遲
- 適合高並發場景

**適用場景**：

- 需要高吞吐量
- 低延遲要求
- 大量使用者同時使用

### 3.2 模型能力比較

| 功能 | Qwen3-8B | Llama3.3-8B | GPT-4o-mini |
| :-- | :-- | :-- | :-- |
| 中文對話 | 非常好 | 好 | 非常好 |
| RAG 融合 | 好 | 好 | 非常好 |
| JSON Tool-calling | 好 (Prompt 可 >90%) | 好 | 非常好 |
| 價格 | 0（本地） | 0 | 付費 |

### 3.3 結論

- 就本專案來說，**Qwen3-8B 或 Llama3.3-8B 已足夠支撐功能**
- 剩下的差距可以用 RAG 強化與 Prompt 導引補起

---

## 4. vLLM + Kubernetes 生產部署

> **注意**：純 Ollama 適合開發 / 小流量。大量併發時，vLLM 的 throughput 與穩定性更適合生產。

### 4.1 高階架構（生產環境）

```
使用者瀏覽器
  ↓ HTTPS
Next.js (Vercel / K8s)
  ↓ REST/WebSocket
FastAPI Gateway (K8s)
  ↓ OpenAI-compatible /v1/chat/completions
vLLM Server (K8s GPU pods, model=Qwen3-8B)
  ↓
PostgreSQL + pgvector (Cloud RDS)  +  Redis (Session/Cache)
```

### 4.2 vLLM 部署範例（Kubernetes YAML）

#### Deployment 配置

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-qwen3
spec:
  replicas: 2
  selector:
    matchLabels:
      app: vllm-qwen3
  template:
    metadata:
      labels:
        app: vllm-qwen3
    spec:
      nodeSelector:
        gpu: "true"
      containers:
      - name: vllm
        image: vllm/vllm-openai:latest
        args:
          - "--model"
          - "Qwen/Qwen2.5-7B-Instruct"
          - "--quantization"
          - "q5_k_m"
          - "--max-model-len"
          - "8192"
          - "--gpu-memory-utilization"
          - "0.9"
        ports:
          - containerPort: 8000
        resources:
          limits:
            nvidia.com/gpu: 1
          requests:
            nvidia.com/gpu: 1
```

#### Service 配置

```yaml
apiVersion: v1
kind: Service
metadata:
  name: vllm-qwen3-svc
spec:
  selector:
    app: vllm-qwen3
  ports:
    - port: 8000
      targetPort: 8000
      protocol: TCP
  type: ClusterIP
```

#### 使用說明

- FastAPI / Node 端只要把 `base_url` 指向 `http://vllm-qwen3-svc:8000/v1` 即可
- 使用 HPA（Horizontal Pod Autoscaler）根據 CPU/GPU 利用率自動調整 replicas

### 4.3 FastAPI Gateway（與現有 OpenAI code 相容）

#### 基本實作

```python
from fastapi import FastAPI
from pydantic import BaseModel
from openai import OpenAI

app = FastAPI()

# 連接到 vLLM 服務
client = OpenAI(
    base_url="http://vllm-qwen3-svc:8000/v1",
    api_key="ollama"  # vLLM不驗證，用假的即可
)

class ChatRequest(BaseModel):
    messages: list
    tools: list | None = None
    stream: bool = False

@app.post("/api/chat")
async def chat(req: ChatRequest):
    # 這裡可以先做 RAG：從 pgvector 取 context
    # ... RAG 邏輯 ...
    
    response = client.chat.completions.create(
        model="Qwen/Qwen2.5-7B-Instruct",
        messages=req.messages,
        tools=req.tools,
        stream=req.stream,
    )
    return response
```

#### 串流回應處理

```python
@app.post("/api/chat/stream")
async def chat_stream(req: ChatRequest):
    stream = client.chat.completions.create(
        model="Qwen/Qwen2.5-7B-Instruct",
        messages=req.messages,
        tools=req.tools,
        stream=True,
    )
    
    async def generate():
        for chunk in stream:
            if chunk.choices[0].delta.content:
                yield f"data: {chunk.choices[0].delta.content}\n\n"
    
    return StreamingResponse(generate(), media_type="text/event-stream")
```

### 4.4 Tool-calling 實作範例

#### 定義 Tools

```python
tools = [
    {
        "type": "function",
        "function": {
            "name": "search_segments",
            "description": "搜尋相關的影片片段",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "搜尋關鍵字"
                    },
                    "city": {
                        "type": "string",
                        "description": "城市名稱（選填）"
                    },
                    "limit": {
                        "type": "integer",
                        "description": "回傳數量上限",
                        "default": 10
                    }
                },
                "required": ["query"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "plan_itinerary",
            "description": "規劃旅遊行程",
            "parameters": {
                "type": "object",
                "properties": {
                    "segments": {
                        "type": "array",
                        "items": {"type": "integer"},
                        "description": "片段 ID 列表"
                    },
                    "days": {
                        "type": "integer",
                        "description": "天數"
                    },
                    "preferences": {
                        "type": "object",
                        "description": "使用者偏好"
                    }
                },
                "required": ["segments", "days"]
            }
        }
    }
]
```

#### 處理 Tool Calls

```python
@app.post("/api/chat")
async def chat(req: ChatRequest):
    response = client.chat.completions.create(
        model="Qwen/Qwen2.5-7B-Instruct",
        messages=req.messages,
        tools=tools,
    )
    
    message = response.choices[0].message
    
    # 檢查是否有 tool calls
    if message.tool_calls:
        tool_results = []
        for tool_call in message.tool_calls:
            if tool_call.function.name == "search_segments":
                # 執行搜尋
                params = json.loads(tool_call.function.arguments)
                results = await search_segments_in_db(params)
                tool_results.append({
                    "tool_call_id": tool_call.id,
                    "role": "tool",
                    "name": "search_segments",
                    "content": json.dumps(results)
                })
            elif tool_call.function.name == "plan_itinerary":
                # 執行行程規劃
                params = json.loads(tool_call.function.arguments)
                itinerary = await plan_itinerary_logic(params)
                tool_results.append({
                    "tool_call_id": tool_call.id,
                    "role": "tool",
                    "name": "plan_itinerary",
                    "content": json.dumps(itinerary)
                })
        
        # 將 tool results 加入 messages 並再次呼叫
        req.messages.append(message)
        req.messages.extend(tool_results)
        
        # 第二次呼叫獲得最終回應
        final_response = client.chat.completions.create(
            model="Qwen/Qwen2.5-7B-Instruct",
            messages=req.messages,
        )
        return final_response
    
    return response
```

---

## 5. 開發環境設定（Ollama）

### 5.1 安裝 Ollama

```bash
# macOS / Linux
curl -fsSL https://ollama.com/install.sh | sh

# Windows
# 下載安裝程式：https://ollama.com/download
```

### 5.2 下載模型

```bash
# 下載 Qwen3-8B
ollama pull qwen2.5:7b-instruct

# 或下載 Llama3.3-8B
ollama pull llama3.3:8b-instruct
```

### 5.3 測試模型

```bash
# 測試對話
ollama run qwen2.5:7b-instruct "你好，請介紹台灣旅遊"

# 測試 JSON 輸出
ollama run qwen2.5:7b-instruct "請輸出 JSON 格式：{\"name\": \"test\"}"
```

### 5.4 Python 整合

```python
import ollama

response = ollama.chat(
    model='qwen2.5:7b-instruct',
    messages=[
        {'role': 'user', 'content': '幫我找台灣兩天一夜的輕鬆小旅行'}
    ]
)

print(response['message']['content'])
```

---

## 6. Prompt 工程建議

### 6.1 中文對話 Prompt

```python
SYSTEM_PROMPT = """你是一個友善的旅遊助手 AI，專門幫助使用者規劃旅遊行程。
你的特點：
1. 用口語、親切的方式回答，像 YouTuber 說話
2. 根據使用者的需求推薦相關的 YouTube 旅遊影片片段
3. 幫助使用者整理成可用的行程表

請用繁體中文回答，語氣要輕鬆、友善。"""
```

### 6.2 Tool-calling Prompt

```python
TOOL_CALLING_PROMPT = """當使用者詢問旅遊相關問題時，請使用以下工具：
1. search_segments: 搜尋相關影片片段
2. plan_itinerary: 規劃行程

請根據使用者需求，適當地呼叫工具並整合結果。"""
```

### 6.3 JSON 輸出 Prompt

```python
JSON_OUTPUT_PROMPT = """請以 JSON 格式輸出，確保格式正確。
範例格式：
{
  "segments": [
    {"id": 1, "title": "安平古堡", "duration": "1:20"}
  ],
  "summary": "找到 3 個相關片段"
}"""
```

---

## 7. 效能優化建議

### 7.1 快取策略

- **對話快取**：使用 Redis 快取常見問題的回答
- **向量快取**：快取常用查詢的 embedding 結果
- **模型快取**：使用 vLLM 的 KV cache 機制

### 7.2 批次處理

- 多個查詢可以批次處理，提升 throughput
- 使用 vLLM 的 continuous batching 功能

### 7.3 量化模型

- 使用量化模型（如 q5_k_m）降低記憶體使用
- 在 GPU 記憶體有限的情況下仍可運行

---

## 8. 監控與除錯

### 8.1 監控指標

- **延遲**：API 回應時間
- **吞吐量**：每秒處理的請求數
- **GPU 使用率**：監控 GPU 資源使用
- **錯誤率**：API 錯誤發生頻率

### 8.2 日誌記錄

```python
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@app.post("/api/chat")
async def chat(req: ChatRequest):
    logger.info(f"收到請求：{req.messages[-1]['content']}")
    # ... 處理邏輯 ...
    logger.info(f"回應時間：{response_time}ms")
```

---

## 9. 遷移檢查清單

### 9.1 開發階段

- [ ] 安裝並測試 Ollama 本地模型
- [ ] 驗證模型的中文對話能力
- [ ] 測試 Tool-calling 功能
- [ ] 驗證 JSON 輸出格式

### 9.2 生產部署

- [ ] 設定 Kubernetes 集群
- [ ] 部署 vLLM 服務
- [ ] 設定 FastAPI Gateway
- [ ] 配置負載平衡與自動擴展
- [ ] 設定監控與告警

### 9.3 測試驗證

- [ ] 功能測試：所有 API 端點正常運作
- [ ] 效能測試：回應時間符合需求
- [ ] 壓力測試：併發處理能力
- [ ] 整合測試：與前端完整流程測試

---

## 10. 參考文件

- [系統需求規格書.md](./系統需求規格書.md) - 系統架構設計
- [部署與成本評估.md](./部署與成本評估.md) - 成本分析
- [開發路線圖.md](./開發路線圖.md) - 開發時程
